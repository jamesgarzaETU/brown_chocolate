{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73711fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/home/ubuntu/anaconda3/bin/python3 python3\n",
    "# ***************************************************************************\n",
    "# Program Title: .../1_process_data_poc.py\n",
    "# Purpose:       Pull Parquet data from S3 and create HTML file for Sim Dashboard\n",
    "# Description:   POC version using Parquet-based ETL pipeline instead of SQL\n",
    "#                SIMS\n",
    "#                ----\n",
    "#                86, # Our Code: We respect one another for Partners\n",
    "#                87, # Our Code: We respect one another for Non-Partners\n",
    "#\n",
    "# Programmer:    Martin McSharry / Claude\n",
    "# Creation Date: 22-December-2024\n",
    "# Input Files:   S3 Parquet files, code_simulation_3_demographic_data.xlsx\n",
    "# Output Files:  index.html, data.pkl\n",
    "# ---------------------------------------------------------------------------\n",
    "# Revision History\n",
    "#   22-Dec-2024 - Created POC version using Parquet pipeline\n",
    "# ***************************************************************************/\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import time, date, datetime, timezone, timedelta\n",
    "import time\n",
    "import csv\n",
    "import sys\n",
    "import timeit\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import shutil\n",
    "from subprocess import call\n",
    "import codecs\n",
    "from shutil import copyfile\n",
    "\n",
    "\n",
    "# Stats modelling\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import scipy\n",
    "\n",
    "# Google Sheets\n",
    "import pygsheets\n",
    "\n",
    "# Logging\n",
    "import pickle\n",
    "\n",
    "# Import ETU functions\n",
    "from skillwell_functions import report, find_ec2, find_rds\n",
    "\n",
    "\n",
    "# Add skillwell_etl to path\n",
    "sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
    "from skillwell_etl.pipeline import ParquetPipeline\n",
    "from skillwell_etl.transform import get_transformed_data_from_parquet\n",
    "from skillwell_etl import backfill, incremental_update\n",
    "\n",
    "\n",
    "# Credentials\n",
    "import json\n",
    "\n",
    "\n",
    "# ----- Script Timer ----->\n",
    "start_tm = timeit.default_timer()\n",
    "script_part_n = 0\n",
    "\n",
    "# -------------------->\n",
    "# ----- Sim data ----->\n",
    "# -------------------->\n",
    "\n",
    "# Configuration for Parquet-based pipeline\n",
    "CUSTOMER = 'mckinsey.skillsims.com'\n",
    "S3_BUCKET = 'etu.appsciences'\n",
    "server = 'mckinsey.skillsims.com'\n",
    "\n",
    "sim_id = [\n",
    "    86, # Our Code: We respect one another for Partners\n",
    "    87, # Our Code: We respect one another for Non-Partners\n",
    "]\n",
    "\n",
    "start_dt = '2025-08-26'\n",
    "end_dt = (date.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "user_groups = None\n",
    "\n",
    "\n",
    "# Project dictionary\n",
    "dict_project = {\n",
    "    (86, 87): 'Our Code: We Respect One Another'}\n",
    "\n",
    "script_part_c = 'Start of: ' + server + ', date:' + end_dt\n",
    "\n",
    "print(script_part_n, ':',  script_part_c)\n",
    "\n",
    "\n",
    "# <--------------------\n",
    "# <----- Sim data -----\n",
    "# <--------------------\n",
    "\n",
    "\n",
    "# ------------------------------------------->\n",
    "# ----- Extract sim data from Parquet ------>\n",
    "# ------------------------------------------->\n",
    "\n",
    "script_part_n += 1\n",
    "script_part_c = 'Extracting Data from Parquet (POC)'\n",
    "\n",
    "print(script_part_n, ':',  script_part_c)\n",
    "\n",
    "try:\n",
    "\n",
    "\n",
    "    # Use the new Parquet-based transformation function\n",
    "    # This replaces: dict_df = extract_data(...)\n",
    "    with open('mckinsey_our_code_we_respect_data.pkl', 'rb') as f:\n",
    "        dict_df = pickle.load(f)\n",
    "\n",
    "\n",
    "\n",
    "except BaseException as e:\n",
    "    print('***ERROR***: ', str(script_part_n), ':',  script_part_c, ':', str(e))\n",
    "    sys.exit(str(script_part_n) +  ': ' +  script_part_c + ' ERRORS!')\n",
    "\n",
    "# <-------------------------------------------\n",
    "# <----- Extract sim data from Parquet ------\n",
    "# <-------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------->\n",
    "# ----- NPS Scores ----->\n",
    "# ---------------------->\n",
    "\n",
    "script_part_n += 1\n",
    "script_part_c = 'NPS Scores'\n",
    "\n",
    "print(script_part_n, ':',  script_part_c)\n",
    "\n",
    "try:\n",
    "    # Order of Sims\n",
    "    dict_sim_order = {}\n",
    "    for i, simid in enumerate(sim_id):\n",
    "        dict_sim_order.update({simid: i})\n",
    "\n",
    "    # Check if survey_responses exists\n",
    "    if 'srv' in dict_df and 'survey_responses' in dict_df['srv'] and dict_df['srv']['survey_responses'] is not None and not dict_df['srv']['survey_responses'].empty:\n",
    "        # Create a temporary DataFrame for easier chaining\n",
    "        dict_df['srv']['survey_responses']['optionvalue'] = dict_df['srv']['survey_responses']['optionvalue'].fillna(0)\n",
    "        temp_df = dict_df['srv']['survey_responses'].copy()\n",
    "\n",
    "        # The .assign() block now uses np.select for conditional logic\n",
    "        processed_df = temp_df.assign(\n",
    "            orderid=0,\n",
    "            question='NPS Score',\n",
    "\n",
    "            answer_nps_num=lambda x: x.apply(\n",
    "                lambda y: int(y['optionvalue']) if pd.notnull(y['answer']) else None, axis=1\n",
    "            ),\n",
    "\n",
    "            answer=lambda x: x['answer_nps_num'].apply(\n",
    "                lambda y: 'Promoter [5-7]' if y in [7, 6, 5] else\n",
    "                            'Passive [3-4]' if y in [4, 3] else\n",
    "                            'Detractor [1-2]' if y in [2, 1] else\n",
    "                            None\n",
    "            ),\n",
    "\n",
    "            bar_color=lambda x: np.select(\n",
    "                [\n",
    "                    x['answer'].str.contains('Promoter', na=False),\n",
    "                    x['answer'].str.contains('Passive', na=False)\n",
    "                ],\n",
    "                [\n",
    "                    '#2aa22a',  # Choice for Promoter\n",
    "                    '#ffffff'   # Choice for Passive\n",
    "                ],\n",
    "                default='#c61110'  # Default for Detractor or any other case\n",
    "            ),\n",
    "\n",
    "            nps_score=lambda x: np.select(\n",
    "                [\n",
    "                    x['answer'].str.contains('Promoter', na=False),\n",
    "                    x['answer'].str.contains('Passive', na=False),\n",
    "                    x['answer'].str.contains('Detractor', na=False)\n",
    "                ],\n",
    "                [\n",
    "                    1 * (x['pct'] / 100),    # Promoter score\n",
    "                    0 * (x['pct'] / 100),    # Passive score\n",
    "                    -1 * (x['pct'] / 100)    # Detractor score\n",
    "                ],\n",
    "                default=None  # Default for any other case\n",
    "            ),\n",
    "\n",
    "            n=lambda x: x.groupby(['simid', 'orderid', 'answer'])['n'].transform('sum'),\n",
    "            pct=lambda x: x.groupby(['simid', 'orderid', 'answer'])['pct'].transform('sum'),\n",
    "\n",
    "            avg_nps_score=lambda x: x.groupby(['simid', 'orderid'])['nps_score'].transform('sum')\n",
    "        )\n",
    "\n",
    "        # Continue with the rest of the chain\n",
    "        final_df = processed_df.filter([\n",
    "            'project', 'simid', 'simname', 'orderid', 'question', 'typeid', 'total',\n",
    "            'answer', 'bar_color', 'n', 'pct', 'avg_nps_score'\n",
    "        ]).drop_duplicates().assign(\n",
    "            sim_order=lambda x: x['simid'].map(dict_sim_order) # .map is faster than .apply here\n",
    "        ).sort_values(\n",
    "            ['sim_order', 'simid', 'orderid', 'answer']\n",
    "        )\n",
    "\n",
    "        # Re-assign the processed data back into the dictionary\n",
    "        dict_df['srv']['survey_responses'] = pd.concat(\n",
    "            [dict_df['srv']['survey_responses'], final_df],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "        # Update the proj_nps DataFrame\n",
    "        #dict_df['srv']['survey_responses'].query('orderid == 0')\n",
    "\n",
    "        temp = pd.concat(\n",
    "            [dict_df['srv']['survey_responses'].query('orderid == 0'), final_df],\n",
    "            ignore_index=True\n",
    "        )\n",
    "        temp['topic_keywords'] = np.nan\n",
    "\n",
    "        # Ensure required columns exist before groupby with correct data types\n",
    "        if 'answerid' not in temp.columns:\n",
    "            temp['answerid'] = np.nan\n",
    "        if 'optionvalue' not in temp.columns:\n",
    "            temp['optionvalue'] = np.nan\n",
    "        if 'dt' not in temp.columns:\n",
    "            temp['dt'] = pd.NaT  # Use NaT for datetime64[ns] type\n",
    "        if 'scale_type' not in temp.columns:\n",
    "            temp['scale_type'] = None\n",
    "        if 'topic_analysis' not in temp.columns:\n",
    "            temp['topic_analysis'] = np.nan  # Use np.nan for float64 type\n",
    "\n",
    "        temp = temp.groupby(['simid', 'answer','project']).agg({\n",
    "            'simname': 'first',\n",
    "            'orderid': 'first',\n",
    "            'question': 'first',\n",
    "            'typeid': 'first',\n",
    "            'total': 'first',\n",
    "            'bar_color': 'first',\n",
    "            'n': 'sum',\n",
    "            'pct': 'sum',\n",
    "            'avg_nps_score': 'mean',\n",
    "            'sim_order': 'first',\n",
    "            'answerid': 'first',\n",
    "            'optionvalue': 'first',\n",
    "            'dt': 'first',\n",
    "            'scale_type': 'first',\n",
    "            'topic_keywords': 'first',\n",
    "            'topic_analysis': 'first'\n",
    "        }).reset_index()\n",
    "        temp = temp[['project', 'simid', 'simname', 'orderid', 'question', 'typeid', 'total','answer', 'bar_color', 'n', 'pct', 'avg_nps_score',\n",
    "                     'sim_order', 'answerid', 'optionvalue', 'dt', 'scale_type', 'topic_keywords', 'topic_analysis']]\n",
    "\n",
    "        # Fix data types to match original SQL output\n",
    "        temp['total'] = temp['total'].astype('float64')\n",
    "        temp['sim_order'] = temp['sim_order'].astype('float64')\n",
    "        # topic_analysis should be float64 (with np.nan for nulls)\n",
    "        temp['topic_analysis'] = temp['topic_analysis'].astype('float64')\n",
    "\n",
    "        dict_df['proj']['proj_nps'] = temp\n",
    "    else:\n",
    "        print(\"No survey_responses data available for NPS calculation\")\n",
    "\n",
    "\n",
    "except BaseException as e:\n",
    "    print('***ERROR***: ', str(script_part_n), ':',  script_part_c, ':', str(e))\n",
    "    sys.exit(str(script_part_n) +  ': ' +  script_part_c + ' ERRORS!')\n",
    "\n",
    "# <----------------------\n",
    "# <----- NPS Scores -----\n",
    "# <----------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------>\n",
    "# ----- Change Skill Names ----->\n",
    "# ------------------------------>\n",
    "\n",
    "script_part_n += 1\n",
    "script_part_c = 'Change Skill Names'\n",
    "\n",
    "print(script_part_n, ':',  script_part_c)\n",
    "\n",
    "try:\n",
    "    if 'sim' in dict_df and 'skill_baseline' in dict_df['sim'] and dict_df['sim']['skill_baseline'] is not None and not dict_df['sim']['skill_baseline'].empty:\n",
    "        dict_df['sim']['skill_baseline'] = dict_df['sim']['skill_baseline']\\\n",
    "        .assign(\n",
    "            skillname = lambda x: x['skillname'].apply(lambda y: \"Overall Performance\" if 'overall performance' in str(y).lower() else y)\n",
    "        )\n",
    "\n",
    "\n",
    "    if 'sim' in dict_df and 'skill_improvement' in dict_df['sim'] and dict_df['sim']['skill_improvement'] is not None and not dict_df['sim']['skill_improvement'].empty:\n",
    "        dict_df['sim']['skill_improvement'] = dict_df['sim']['skill_improvement']\\\n",
    "        .assign(\n",
    "            skillname = lambda x: x['skillname'].apply(lambda y: \"Overall Performance\" if 'overall performance' in str(y).lower() else y)\n",
    "        )\n",
    "\n",
    "\n",
    "    if 'dmg' in dict_df and 'dmg_skill_baseline' in dict_df['dmg'] and dict_df['dmg']['dmg_skill_baseline'] is not None and not dict_df['dmg']['dmg_skill_baseline'].empty:\n",
    "        dict_df['dmg']['dmg_skill_baseline'] = dict_df['dmg']['dmg_skill_baseline']\\\n",
    "        .assign(\n",
    "            skillname = lambda x: x['skillname'].apply(lambda y: \"Overall Performance\" if 'overall performance' in str(y).lower() else y)\n",
    "        )\n",
    "\n",
    "\n",
    "except BaseException as e:\n",
    "    print('***ERROR***: ', str(script_part_n), ':',  script_part_c, ':', str(e))\n",
    "    sys.exit(str(script_part_n) +  ': ' +  script_part_c + ' ERRORS!')\n",
    "\n",
    "\n",
    "# <------------------------------\n",
    "# <----- Change Skill Names -----\n",
    "# <------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------->\n",
    "# ----- Create HTML file ----->\n",
    "# ---------------------------->\n",
    "\n",
    "script_part_n += 1\n",
    "script_part_c = 'Create HTML file'\n",
    "\n",
    "print(script_part_n, ':',  script_part_c)\n",
    "\n",
    "try:\n",
    "\n",
    "    index_html = report(\n",
    "        dict_df,\n",
    "        dict_project=dict_project,\n",
    "        start_date=start_dt,\n",
    "        end_date=end_dt,\n",
    "        mckinsey=True,\n",
    "    )\n",
    "\n",
    "    # Create HTML File (POC output)\n",
    "    html_output_path = os.path.join('index_poc.html')\n",
    "    with open(html_output_path, 'w') as outfile:\n",
    "        outfile.write(index_html)\n",
    "    print.info(f\"Saved POC HTML to {html_output_path}\")\n",
    "\n",
    "\n",
    "except BaseException as e:\n",
    "    print('***ERROR***: ', str(script_part_n), ':',  script_part_c, ':', str(e))\n",
    "    sys.exit(str(script_part_n) +  ': ' +  script_part_c + ' ERRORS!')\n",
    "\n",
    "# <----------------------------\n",
    "# <----- Create HTML file -----\n",
    "# <----------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Script Duration: ', str(round((timeit.default_timer() - start_tm)/60, 2)), ' minutes')\n",
    "print(\"<----- SCRIPT RUN SUCCESSFUL -----\")\n",
    "os._exit(os.EX_OK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f37662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26459e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fc8f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbfd286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
